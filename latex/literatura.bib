@inproceedings{brants2007large,
  author    = {Thorsten Brants and Ashok C. Popat and Peng Xu and Franz J. Och and Jeffrey Dean},
  title     = {Large Language Models in Machine Translation},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
  year      = {2007},
  pages     = {858--867},
  address   = {Prague, Czech Republic},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{yong2023authors,
  author    = {Yong Zheng},
  title     = {Authors Info \& Claims},
  booktitle = {SIGITE '23: Proceedings of the 24th Annual Conference on Information Technology Education},
  month     = {October},
  year      = {2023},
  pages     = {66--72},
  doi       = {10.1145/3585059.3611431}
}
@inproceedings{baidoo2023education,
  author    = {D. BAİDOO-ANU and L. OWUSU ANSAH},
  title     = {Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning},
  journal   = {Journal of AI},
  volume    = {7},
  number    = {1},
  pages     = {52--62},
  year      = {2023},
  doi       = {10.61969/jai.1337500},
  url       = {https://doi.org/10.61969/jai.1337500}
}

@Article{info:doi/10.2196/50638,
author="Mesk{\'o}, Bertalan",
title="Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
journal="J Med Internet Res",
year="2023",
month="Oct",
day="4",
volume="25",
pages="e50638",
keywords="artificial intelligence; AI; digital health; future; technology; ChatGPT; GPT-4; large language models; language model; LLM; prompt; prompts; prompt engineering; AI tool; engineering; healthcare professional; decision-making; LLMs; chatbot; chatbots; conversational agent; conversational agents; NLP; natural language processing",
issn="1438-8871",
doi="10.2196/50638",
url="https://www.jmir.org/2023/1/e50638",
url="https://doi.org/10.2196/50638",
url="http://www.ncbi.nlm.nih.gov/pubmed/37792434"
}

@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and Suchir Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374},
  url={https://api.semanticscholar.org/CorpusID:235755472}
}


@article{zhang2003machine,
  author    = {D. Zhang and J. J. Tsai},
  title     = {Machine Learning and Software Engineering},
  journal   = {Software Quality Journal},
  volume    = {11},
  number    = {2},
  pages     = {87--119},
  year      = {2003},
  month     = {Jun},
  doi       = {10.1023/A:1023760326768},
  url       = {https://doi.org/10.1023/A:1023760326768}
}

@INPROCEEDINGS{9833571,
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions}, 
  year={2022},
  volume={},
  number={},
  pages={754-768},
  keywords={Privacy;Codes;Computational modeling;Keyboards;Computer crime;Open source software;Software development management;Cybersecurity;Artificial Intelligence (AI);code generation;Common Weakness Enumerations (CWEs)},
  doi={10.1109/SP46214.2022.9833571}}


@Article{app13095783,
AUTHOR = {Rahman, Md. Mostafizer and Watanobe, Yutaka},
TITLE = {ChatGPT for Education and Research: Opportunities, Threats, and Strategies},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {9},
ARTICLE-NUMBER = {5783},
URL = {https://www.mdpi.com/2076-3417/13/9/5783},
ISSN = {2076-3417},
ABSTRACT = {In recent years, the rise of advanced artificial intelligence technologies has had a profound impact on many fields, including education and research. One such technology is ChatGPT, a powerful large language model developed by OpenAI. This technology offers exciting opportunities for students and educators, including personalized feedback, increased accessibility, interactive conversations, lesson preparation, evaluation, and new ways to teach complex concepts. However, ChatGPT poses different threats to the traditional education and research system, including the possibility of cheating on online exams, human-like text generation, diminished critical thinking skills, and difficulties in evaluating information generated by ChatGPT. This study explores the potential opportunities and threats that ChatGPT poses to overall education from the perspective of students and educators. Furthermore, for programming learning, we explore how ChatGPT helps students improve their programming skills. To demonstrate this, we conducted different coding-related experiments with ChatGPT, including code generation from problem descriptions, pseudocode generation of algorithms from texts, and code correction. The generated codes are validated with an online judge system to evaluate their accuracy. In addition, we conducted several surveys with students and teachers to find out how ChatGPT supports programming learning and teaching. Finally, we present the survey results and analysis.},
DOI = {10.3390/app13095783}
}

@inproceedings{10.1145/3520312.3534862,
author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
title = {A systematic evaluation of large language models of code},
year = {2022},
isbn = {9781450392730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520312.3534862},
doi = {10.1145/3520312.3534862},
abstract = {Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language
modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming
languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.
We have an online appendix at https://arxiv.org/abs/2202.13169.},
booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
pages = {1–10},
numpages = {10},
keywords = {code generation, code language model, evaluation, open-source, pretraining},
location = {San Diego, CA, USA},
series = {MAPS 2022}
}

@article{MORADIDAKHEL2023111734,
title = {GitHub Copilot AI pair programmer: Asset or Liability?},
journal = {Journal of Systems and Software},
volume = {203},
pages = {111734},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111734},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001292},
author = {Arghavan {Moradi Dakhel} and Vahid Majdinasab and Amin Nikanjam and Foutse Khomh and Michel C. Desmarais and Zhen Ming (Jack) Jiang},
keywords = {Code completion, Language model, GitHub copilot, Testing},
abstract = {Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot’s proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans’ solutions is greater than Copilot’s suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans’ contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise.}
}

@article{yetistiren2023evaluating,
  title={Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT},
  author={Yetiştiren, Burak and others},
  journal={arXiv preprint arXiv:2304.10778},
  year={2023},
  url={https://doi.org/10.48550/arXiv.2304.10778}
}

@article{KASNECI2023102274,
title = {ChatGPT for good? On opportunities and challenges of large language models for education},
journal = {Learning and Individual Differences},
volume = {103},
pages = {102274},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2023.102274},
url = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
author = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
keywords = {Large language models, Artificial intelligence, Education, Educational technologies},
abstract = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{rudolph2023war,
  title={War of the chatbots: Bard, Bing Chat, ChatGPT, Ernie and beyond. The new AI gold rush and its impact on higher education},
  author={Rudolph, J{\"u}rgen and Tan, Shannon and Tan, Samson},
  journal={Journal of Applied Learning and Teaching},
  volume={6},
  number={1},
  year={2023}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{datacamp_attention_2024,
    title = {Attention Mechanism in Large Language Models: An Intuition},
    author = {DataCamp},
    year = {2024},
    url = {https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition?dc_referrer=https%3A%2F%2Fwww.google.com%2F},
    note = {Accessed: 2024-06-13}
}


@misc{wang2024prompt,
      title={Prompt Engineering for Healthcare: Methodologies and Applications}, 
      author={Jiaqi Wang and Enze Shi and Sigang Yu and Zihao Wu and Chong Ma and Haixing Dai and Qiushi Yang and Yanqing Kang and Jinru Wu and Huawen Hu and Chenxi Yue and Haiyang Zhang and Yiheng Liu and Yi Pan and Zhengliang Liu and Lichao Sun and Xiang Li and Bao Ge and Xi Jiang and Dajiang Zhu and Yixuan Yuan and Dinggang Shen and Tianming Liu and Shu Zhang},
      year={2024},
      eprint={2304.14670},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@misc{openai_codex,
    author       = {{OpenAI}},
    title        = {OpenAI Codex},
    year         = 2024,
    url          = {https://openai.com/index/openai-codex/},
    note         = {Accessed: 2024-06-17}
}

@misc{openai_chatgpt,
  author       = {OpenAI},
  title        = {ChatGPT},
  howpublished = {\url{https://openai.com/chatgpt/}},
  year         = 2024,
  note         = {Accessed: 2024-06-29}
}

@misc{saasworthy_codewhisperer,
  author       = {SaaSworthy},
  title        = {Amazon CodeWhisperer},
  howpublished = {\url{https://www.saasworthy.com/product/amazon-codewhisperer}},
  year         = 2024,
  note         = {Accessed: 2024-06-29}
}

@misc{github_copilot,
  author       = {GitHub},
  title        = {GitHub Copilot},
  howpublished = {\url{https://github.com/features/copilot/}},
  year         = 2024,
  note         = {Accessed: 2024-06-29}
}

@misc{github_copilot_chat,
  author       = {GitHub},
  title        = {GitHub Copilot Chat},
  howpublished = {\url{https://github.blog/2023-12-29-github-copilot-chat-now-generally-available-for-organizations-and-individuals//}},
  year         = 2023,
  note         = {Accessed: 2024-06-30}
}

@misc{Sundqvist1866649,
   author = {Sundqvist, Eric},
   institution = {Linnaeus University, Department of computer science and media technology (CM)},
   school = {Linnaeus University, Department of computer science and media technology (CM)},
   title = {AI-Assisted Unit Testing : Empirical Insights into GitHub Copilot Chat's Effectiveness and Collaborative Benefits},
  howpublished = {\url{https://www.diva-portal.org/smash/record.jsf?pid=diva2:1866649//}},
   year = 2024,
  note         = {Accessed: 2024-06-30}
}
@book{aggarwal2005software,
  title={Software engineering},
  author={Aggarwal, KK},
  year={2005},
  publisher={New Age International}
}

@INPROCEEDINGS{7577432,
  author={Ebert, Christof and Kuhrmann, Marco and Prikladnicki, Rafael},
  booktitle={2016 IEEE 11th International Conference on Global Software Engineering (ICGSE)}, 
  title={Global Software Engineering: Evolution and Trends}, 
  year={2016},
  volume={},
  number={},
  pages={144-153},
  keywords={Industries;Collaboration;Software engineering;Indexes;Software;Market research;Organizations;global software engineering;GSE;near-shoring;outsourcing;offshoring;longitudinal study;ICGSE},
  doi={10.1109/ICGSE.2016.19}}

@article{AlSaqqa2020AgileSD,
  title={Agile Software Development: Methodologies and Trends},
  author={Samar Al-Saqqa and Samer Sawalha and Heba Abdelnabi},
  journal={Int. J. Interact. Mob. Technol.},
  year={2020},
  volume={14},
  pages={246-270},
  url={https://api.semanticscholar.org/CorpusID:225548331}
}

@article{DBLP:journals/corr/abs-1709-08439,
  author       = {Pekka Abrahamsson and
                  Outi Salo and
                  Jussi Ronkainen and
                  Juhani Warsta},
  title        = {Agile Software Development Methods: Review and Analysis},
  journal      = {CoRR},
  volume       = {abs/1709.08439},
  year         = {2017},
  url          = {http://arxiv.org/abs/1709.08439},
  eprinttype    = {arXiv},
  eprint       = {1709.08439},
  timestamp    = {Mon, 13 Aug 2018 16:48:55 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1709-08439.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Article{info:doi/10.2196/50638,
author="Mesk{\'o}, Bertalan",
title="Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
journal="J Med Internet Res",
year="2023",
month="Oct",
day="4",
volume="25",
pages="e50638",
keywords="artificial intelligence; AI; digital health; future; technology; ChatGPT; GPT-4; large language models; language model; LLM; prompt; prompts; prompt engineering; AI tool; engineering; healthcare professional; decision-making; LLMs; chatbot; chatbots; conversational agent; conversational agents; NLP; natural language processing",
issn="1438-8871",
doi="10.2196/50638",
url="https://www.jmir.org/2023/1/e50638",
url="https://doi.org/10.2196/50638",
url="http://www.ncbi.nlm.nih.gov/pubmed/37792434"
}

@misc{tian2023chatgptultimateprogrammingassistant,
      title={Is ChatGPT the Ultimate Programming Assistant -- How far is it?}, 
      author={Haoye Tian and Weiqi Lu and Tsz On Li and Xunzhu Tang and Shing-Chi Cheung and Jacques Klein and Tegawendé F. Bissyandé},
      year={2023},
      eprint={2304.11938},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2304.11938}, 
}

@article{Meyer2023,
  author    = {Meyer, J.G. and Urbanowicz, R.J. and Martin, P.C.N. and others},
  title     = {ChatGPT and large language models in academia: opportunities and challenges},
  journal   = {BioData Mining},
  volume    = {16},
  number    = {20},
  year      = {2023},
  url       = {https://doi.org/10.1186/s13040-023-00339-9},
  doi       = {10.1186/s13040-023-00339-9},
  note      = {Published 13 July 2023}
}

@article{DBLP:journals/corr/abs-2107-03374,
  author       = {Mark Chen and
                  Jerry Tworek and
                  Heewoo Jun and
                  Qiming Yuan and
                  Henrique Pond{\'{e}} de Oliveira Pinto and
                  Jared Kaplan and
                  Harrison Edwards and
                  Yuri Burda and
                  Nicholas Joseph and
                  Greg Brockman and
                  Alex Ray and
                  Raul Puri and
                  Gretchen Krueger and
                  Michael Petrov and
                  Heidy Khlaaf and
                  Girish Sastry and
                  Pamela Mishkin and
                  Brooke Chan and
                  Scott Gray and
                  Nick Ryder and
                  Mikhail Pavlov and
                  Alethea Power and
                  Lukasz Kaiser and
                  Mohammad Bavarian and
                  Clemens Winter and
                  Philippe Tillet and
                  Felipe Petroski Such and
                  Dave Cummings and
                  Matthias Plappert and
                  Fotios Chantzis and
                  Elizabeth Barnes and
                  Ariel Herbert{-}Voss and
                  William Hebgen Guss and
                  Alex Nichol and
                  Alex Paino and
                  Nikolas Tezak and
                  Jie Tang and
                  Igor Babuschkin and
                  Suchir Balaji and
                  Shantanu Jain and
                  William Saunders and
                  Christopher Hesse and
                  Andrew N. Carr and
                  Jan Leike and
                  Joshua Achiam and
                  Vedant Misra and
                  Evan Morikawa and
                  Alec Radford and
                  Matthew Knight and
                  Miles Brundage and
                  Mira Murati and
                  Katie Mayer and
                  Peter Welinder and
                  Bob McGrew and
                  Dario Amodei and
                  Sam McCandlish and
                  Ilya Sutskever and
                  Wojciech Zaremba},
  title        = {Evaluating Large Language Models Trained on Code},
  journal      = {CoRR},
  volume       = {abs/2107.03374},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.03374},
  eprinttype    = {arXiv},
  eprint       = {2107.03374},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-03374.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{minaee2024largelanguagemodelssurvey,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2024},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06196}, 
}

 @article{Hadi_2023,
title={A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage},
url={http://dx.doi.org/10.36227/techrxiv.23589741.v1},
DOI={10.36227/techrxiv.23589741.v1},
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
author={Hadi, Muhammad Usman and tashi, qasem al and Qureshi, Rizwan and Shah, Abbas and muneer, amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali},
year={2023},
month=jul }

@misc{dodatneNaloge,
  title        = {Dodatne Naloge},
  author       = {Luka Fürst},
  year         = {2024},
  url          = {https://ucilnica.fri.uni-lj.si/pluginfile.php/163337/course/section/6561/dodatneNaloge.pdf},
  note         = {Accessed: 2024-08-10},
  howpublished = {PDF document, available online},
}

@article{Wasserman_Engineering,
author = {Wasserman, Anthony},
year = {1996},
month = {12},
pages = {23 - 31},
title = {Toward a discipline of software engineering},
volume = {13},
journal = {Software, IEEE},
doi = {10.1109/52.542291}
}

@online{mercun2012,
    author = {Tanja Merčun},
    title = {Understanding Metadata: Lecture Slide Presentation},
    year = {2012},
    url = {https://slideplayer.com/slide/11457221/},
    urldate = {2024-09-13}
}

@article{randell19961968,
  title={The 1968/69 nato software engineering reports},
  author={Randell, Brian},
  journal={History of software engineering},
  volume={37},
  year={1996}
}

@inproceedings{boehm2006view,
  title={A view of 20th and 21st century software engineering},
  author={Boehm, Barry},
  booktitle={Proceedings of the 28th international conference on Software engineering},
  pages={12--29},
  year={2006}
}

@article{humphrey1989managing,
  title={Managing the Software Process},
  author={Humphrey, WS},
  journal={Addison-Wesley google schola},
  volume={2},
  pages={73--79},
  year={1989}
}

@misc{schach1996classical,
  title={Classical and Object-Oriented Software Engineering},
  author={Schach Stephen, R},
  year={1996},
  publisher={Richard D. Irwin}
}

@article{alshamrani2015comparison,
  title={A comparison between three SDLC models waterfall model, spiral model, and Incremental/Iterative model},
  author={Alshamrani, Adel and Bahattab, Abdullah},
  journal={International Journal of Computer Science Issues (IJCSI)},
  volume={12},
  number={1},
  pages={106},
  year={2015}
}

@book{highsmith2009agile,
  title={Agile project management: creating innovative products},
  author={Highsmith, Jim},
  year={2009},
  publisher={Pearson education}
}

@article{bavishi2019autopandas,
  title={AutoPandas: neural-backed generators for program synthesis},
  author={Bavishi, Rohan and Lemieux, Caroline and Fox, Roy and Sen, Koushik and Stoica, Ion},
  journal={Proceedings of the ACM on Programming Languages},
  volume={3},
  number={OOPSLA},
  pages={1--27},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@inproceedings{vaithilingam2022expectation,
  title={Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models},
  author={Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L},
  booktitle={Chi conference on human factors in computing systems extended abstracts},
  pages={1--7},
  year={2022}
}

@inproceedings{wan2018improving,
  title={Improving automatic source code summarization via deep reinforcement learning},
  author={Wan, Yao and Zhao, Zhou and Yang, Min and Xu, Guandong and Ying, Haochao and Wu, Jian and Yu, Philip S},
  booktitle={Proceedings of the 33rd ACM/IEEE international conference on automated software engineering},
  pages={397--407},
  year={2018}
}

@inproceedings{hu2019re,
  title={Re-factoring based program repair applied to programming assignments},
  author={Hu, Yang and Ahmed, Umair Z and Mechtaev, Sergey and Leong, Ben and Roychoudhury, Abhik},
  booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={388--398},
  year={2019},
  organization={IEEE}
}

@article{wu2023brief,
  title={A brief overview of ChatGPT: The history, status quo and potential future development},
  author={Wu, Tianyu and He, Shizhu and Liu, Jingping and Sun, Siqi and Liu, Kang and Han, Qing-Long and Tang, Yang},
  journal={IEEE/CAA Journal of Automatica Sinica},
  volume={10},
  number={5},
  pages={1122--1136},
  year={2023},
  publisher={IEEE}
}

@article{alkaissi2023artificial,
  title={Artificial hallucinations in ChatGPT: implications in scientific writing},
  author={Alkaissi, Hussam and McFarlane, Samy I},
  journal={Cureus},
  volume={15},
  number={2},
  year={2023},
  publisher={Cureus Inc.}
}

@misc{tung2023chatgpt,
  title={ChatGPT can write code. Now researchers say it’s good at fixing bugs, too. ZDNet},
  author={Tung, Liam},
  year={2023}
}

@online{aws_developer,
    title     = {AWS Developer Center},
    url       = {https://aws.amazon.com/q/developer/},
    note      = {Accessed: 24-09-2024},
    year      = {2024}
}



