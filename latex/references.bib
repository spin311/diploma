@inproceedings{yetistiren2023evaluating,
  author       = {Yetiştiren, B. and Özsoy, I. and Ayerdem, M. and Tüzün, E.},
  title        = {Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT},
  year         = {2023},
  month        = {April},
  day          = {21},
  howpublished = {Pridobljeno 28. 02. 2024 iz: \url{https://arxiv.org/abs/2304.10778}}
}

@inproceedings{brants2007large,
  author    = {Thorsten Brants and Ashok C. Popat and Peng Xu and Franz J. Och and Jeffrey Dean},
  title     = {Large Language Models in Machine Translation},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
  year      = {2007},
  pages     = {858--867},
  address   = {Prague, Czech Republic},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{yong2023authors,
  author    = {Yong Zheng},
  title     = {Authors Info \& Claims},
  booktitle = {SIGITE '23: Proceedings of the 24th Annual Conference on Information Technology Education},
  month     = {October},
  year      = {2023},
  pages     = {66--72},
  doi       = {10.1145/3585059.3611431}
}
@inproceedings{baidoo2023education,
  author    = {D. BAİDOO-ANU and L. OWUSU ANSAH},
  title     = {Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning},
  journal   = {Journal of AI},
  volume    = {7},
  number    = {1},
  pages     = {52--62},
  year      = {2023},
  doi       = {10.61969/jai.1337500},
  url       = {https://doi.org/10.61969/jai.1337500}
}

@Article{info:doi/10.2196/50638,
author="Mesk{\'o}, Bertalan",
title="Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
journal="J Med Internet Res",
year="2023",
month="Oct",
day="4",
volume="25",
pages="e50638",
keywords="artificial intelligence; AI; digital health; future; technology; ChatGPT; GPT-4; large language models; language model; LLM; prompt; prompts; prompt engineering; AI tool; engineering; healthcare professional; decision-making; LLMs; chatbot; chatbots; conversational agent; conversational agents; NLP; natural language processing",
issn="1438-8871",
doi="10.2196/50638",
url="https://www.jmir.org/2023/1/e50638",
url="https://doi.org/10.2196/50638",
url="http://www.ncbi.nlm.nih.gov/pubmed/37792434"
}

@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and Suchir Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374},
  url={https://api.semanticscholar.org/CorpusID:235755472}
}


@article{zhang2003machine,
  author    = {D. Zhang and J. J. Tsai},
  title     = {Machine Learning and Software Engineering},
  journal   = {Software Quality Journal},
  volume    = {11},
  number    = {2},
  pages     = {87--119},
  year      = {2003},
  month     = {Jun},
  doi       = {10.1023/A:1023760326768},
  url       = {https://doi.org/10.1023/A:1023760326768}
}

@INPROCEEDINGS{9833571,
  author={Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and Dolan-Gavitt, Brendan and Karri, Ramesh},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions}, 
  year={2022},
  volume={},
  number={},
  pages={754-768},
  keywords={Privacy;Codes;Computational modeling;Keyboards;Computer crime;Open source software;Software development management;Cybersecurity;Artificial Intelligence (AI);code generation;Common Weakness Enumerations (CWEs)},
  doi={10.1109/SP46214.2022.9833571}}


@Article{app13095783,
AUTHOR = {Rahman, Md. Mostafizer and Watanobe, Yutaka},
TITLE = {ChatGPT for Education and Research: Opportunities, Threats, and Strategies},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {9},
ARTICLE-NUMBER = {5783},
URL = {https://www.mdpi.com/2076-3417/13/9/5783},
ISSN = {2076-3417},
ABSTRACT = {In recent years, the rise of advanced artificial intelligence technologies has had a profound impact on many fields, including education and research. One such technology is ChatGPT, a powerful large language model developed by OpenAI. This technology offers exciting opportunities for students and educators, including personalized feedback, increased accessibility, interactive conversations, lesson preparation, evaluation, and new ways to teach complex concepts. However, ChatGPT poses different threats to the traditional education and research system, including the possibility of cheating on online exams, human-like text generation, diminished critical thinking skills, and difficulties in evaluating information generated by ChatGPT. This study explores the potential opportunities and threats that ChatGPT poses to overall education from the perspective of students and educators. Furthermore, for programming learning, we explore how ChatGPT helps students improve their programming skills. To demonstrate this, we conducted different coding-related experiments with ChatGPT, including code generation from problem descriptions, pseudocode generation of algorithms from texts, and code correction. The generated codes are validated with an online judge system to evaluate their accuracy. In addition, we conducted several surveys with students and teachers to find out how ChatGPT supports programming learning and teaching. Finally, we present the survey results and analysis.},
DOI = {10.3390/app13095783}
}

@inproceedings{10.1145/3520312.3534862,
author = {Xu, Frank F. and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
title = {A systematic evaluation of large language models of code},
year = {2022},
isbn = {9781450392730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520312.3534862},
doi = {10.1145/3520312.3534862},
abstract = {Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language
modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming
languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area.
We have an online appendix at https://arxiv.org/abs/2202.13169.},
booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
pages = {1–10},
numpages = {10},
keywords = {code generation, code language model, evaluation, open-source, pretraining},
location = {San Diego, CA, USA},
series = {MAPS 2022}
}




